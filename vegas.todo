Right now:

- test_vegas.py should test pickling.  

For later:

- If sigf_list gets too large, switch to a mode where sigf 
  is computed using self.neval_hcube  evaluations per hypercube, whic
  evaluations are then discarded (to avoid bad feedback, resulting in biases).
  The extra evaluations would cause at most a factor of 2
  increase in run time. This would be worth it to gain a factor of 2 or 3
  smaller error. This is like our original attempt at the new vegas, but is
  more acceptable as a fall back position for very large values of neval.

- Make multivegas --- integrand is a vector of different integrands.

- replace sigf_list with a double list, containing h-cube number and sigf, 
  so that we only save sigf data for places where sigf is unusually large.
  See branch mem-opt for a start on this.

- Think about roundoff error in the accumulation of results. Could introduce
  vec_mean and vec_var so accumulate whole vectors before folding into 
  the total answer. Could keep vec_mean and vec_var in arrays that 
  are summed intelligently at the end. That would be the best but involves
  a lot of storage possibly, although less than storage in sigf_list if
  nhcube_vec > 2. See math.fsum documentation in Python manual, and 
  also: http://code.activestate.com/recipes/393090/ TOO SLOW -- increases
  time in Kinoshita's 5-dim example by factor of 6. vec_mean and vec_var
  may best we can do. Not too bad since do sums for each hcube as well. Doing
  vec_mean and vec_var would only help, however, if nhcube_vec was a lot larger
  than 30. This is a non issue for the vars since they are all positive. It
  feels like a non-issue for the means as well since the likelihood of two
  very large terms cancelling in the sum is very small for numbers coming
  from Monte Carlo.

- Do we want a max_neval_hcube, to protect from too much memory? Also could
  limit nhcube, for the same reason.

